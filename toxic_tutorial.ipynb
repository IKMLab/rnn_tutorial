{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zake7749/.local/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Load the packages\n",
    "########################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Bidirectional, CuDNNGRU, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Define the hyper parameters\n",
    "########################################\n",
    "path = 'Dataset/'\n",
    "TRAIN_DATA_FILE=path + 'train.csv'\n",
    "TEST_DATA_FILE=path + 'test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training / testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Load the training / testing set with pandas csv format\n",
    "########################################\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quick view of training set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"A quick view of training set\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quick view of testing set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"A quick view of testing set\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 336613 unique tokens\n",
      "Shape of data tensor: (159571, 100)\n",
      "Shape of label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 100)\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## Text pre-processing and cleaning\n",
    "########################################\n",
    "print('Processing text dataset')\n",
    "from collections import defaultdict\n",
    "\n",
    "# regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "# regex to replace all numeric\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def clean_text(text, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"i’m\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = replace_numbers.sub('', text)\n",
    "    text = special_character_removal.sub('',text)\n",
    "\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "list_sentences_train = train_df[\"comment_text\"].fillna(\"no comment\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_labels = train_df[list_classes].values\n",
    "list_sentences_test = test_df[\"comment_text\"].fillna(\"no comment\").values\n",
    "\n",
    "train_comments = [clean_text(text) for text in list_sentences_train]\n",
    "test_comments = [clean_text(text) for text in list_sentences_test]\n",
    "\n",
    "# Create a tokenize, which transforms a sentence to a list of ids\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "\n",
    "# Build the relation between words and ids \n",
    "tokenizer.fit_on_texts(train_comments + test_comments)\n",
    "\n",
    "# Transform training/testing sentences to training/testing sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set with word format:\n",
      "['explanationwhy the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now    ', 'd aww  he matches this background colour i am seemingly stuck with thanks talk  january   utc', 'hey man i am really not trying to edit war it just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info ']\n"
     ]
    }
   ],
   "source": [
    "print(\"training set with word format:\")\n",
    "print(train_comments[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set with index format\n",
      "[[1, 133, 125, 174, 29, 679, 4231, 9277, 1127, 83, 348, 46, 83, 12, 13348, 51, 6444, 15, 60, 2499, 146, 3, 2687, 33, 110, 1171, 15522, 2502, 6, 52, 20, 12, 246, 1, 422, 31, 1, 56, 30, 138, 3, 40, 3754, 88], [349, 7779, 48, 2771, 14, 447, 3575, 3, 40, 4622, 2532, 22, 94, 56, 951, 239], [443, 365, 3, 40, 119, 12, 255, 2, 79, 308, 10, 51, 9, 14, 555, 8, 2335, 504, 481, 102, 6, 573, 2, 42, 312, 133, 361, 4, 29, 56, 30, 48, 183, 2, 437, 59, 37, 1, 2295, 91, 1, 694, 464]]\n"
     ]
    }
   ],
   "source": [
    "print(\"training set with index format\")\n",
    "print(train_sequences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set with padded(by zero) index format:\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     1   133   125   174\n",
      "     29   679  4231  9277  1127    83   348    46    83    12 13348    51\n",
      "   6444    15    60  2499   146     3  2687    33   110  1171 15522  2502\n",
      "      6    52    20    12   246     1   422    31     1    56    30   138\n",
      "      3    40  3754    88]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    349  7779    48  2771    14   447  3575     3    40  4622  2532    22\n",
      "     94    56   951   239]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0   443   365     3\n",
      "     40   119    12   255     2    79   308    10    51     9    14   555\n",
      "      8  2335   504   481   102     6   573     2    42   312   133   361\n",
      "      4    29    56    30    48   183     2   437    59    37     1  2295\n",
      "     91     1   694   464]]\n"
     ]
    }
   ],
   "source": [
    "print(\"training set with padded(by zero) index format:\")\n",
    "print(train_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## Define the text rnn model structure\n",
    "########################################\n",
    "def get_text_rnn():\n",
    "    recurrent_units = 48\n",
    "    dense_units = 32\n",
    "    output_units = 6\n",
    "    \n",
    "    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(input_layer)\n",
    "    \n",
    "    x = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(embedding_layer)\n",
    "    x = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=False))(x)\n",
    "    \n",
    "    x = Dense(dense_units, activation=\"relu\")(x)\n",
    "    output_layer = Dense(output_units, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## Define the text cnn model structure\n",
    "########################################\n",
    "def get_text_cnn():\n",
    "    filter_nums = 120\n",
    "    dense_units = 72\n",
    "    output_units = 6\n",
    "    \n",
    "    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,)(input_layer)\n",
    "        \n",
    "    conv_0 = Conv1D(filter_nums, 2, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedding_layer)\n",
    "    conv_1 = Conv1D(filter_nums, 3, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedding_layer)\n",
    "    conv_2 = Conv1D(filter_nums, 4, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "\n",
    "    merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2])\n",
    "    h1 = Dense(units=dense_units, activation=\"relu\")(merged_tensor)\n",
    "    output = Dense(units=output_units, activation='sigmoid')(h1)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-Fold Cross Valiadtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## Construct the cross-validation framework\n",
    "########################################\n",
    "\n",
    "def _train_model_by_logloss(model, batch_size, train_x, train_y, val_x, val_y, fold_id):\n",
    "    # set an early stopping checker.\n",
    "    # the training phase would stop when validation log loss decreases continuously for `patience` rounds. \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    bst_model_path = \"ToxicModel\" + str(fold_id) + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    \n",
    "    # training on given fold data\n",
    "    hist = model.fit(train_x, train_y,\n",
    "        validation_data=(val_x, val_y),\n",
    "        epochs=50, batch_size=batch_size, shuffle=True,\n",
    "        callbacks=[early_stopping, model_checkpoint])\n",
    "    \n",
    "    # get the minimal validation log loss on this fold\n",
    "    bst_val_score = min(hist.history['val_loss'])\n",
    "\n",
    "    # return the model with best weight, best fold-val score\n",
    "    return model, bst_val_score\n",
    "\n",
    "def train_folds(X, y, fold_count, batch_size, get_model_func):\n",
    "    fold_size = len(X) // fold_count\n",
    "    models = []\n",
    "    score = 0\n",
    "    \n",
    "    # split the whole dataset to `fold_count` fold, and train our model on each fold\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(X)\n",
    "\n",
    "        # Generate the train/val data on fold i\n",
    "        train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "        val_x = X[fold_start:fold_end]\n",
    "        val_y = y[fold_start:fold_end]\n",
    "    \n",
    "        print(\"Training on fold #\", fold_id)\n",
    "        model, bst_val_score = _train_model_by_logloss(get_model_func(), batch_size, train_x, train_y, val_x, val_y, fold_id)\n",
    "        score += bst_val_score\n",
    "        models.append(model)\n",
    "    return models, score / fold_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold # 0\n",
      "Train on 79786 samples, validate on 79785 samples\n",
      "Epoch 1/50\n",
      "79786/79786 [==============================] - 39s 494us/step - loss: 0.0935 - acc: 0.9702 - val_loss: 0.0530 - val_acc: 0.9810\n",
      "Epoch 2/50\n",
      "79786/79786 [==============================] - 37s 469us/step - loss: 0.0431 - acc: 0.9840 - val_loss: 0.0500 - val_acc: 0.9818\n",
      "Epoch 3/50\n",
      "79786/79786 [==============================] - 38s 471us/step - loss: 0.0316 - acc: 0.9878 - val_loss: 0.0520 - val_acc: 0.9813\n",
      "Epoch 4/50\n",
      "79786/79786 [==============================] - 38s 475us/step - loss: 0.0216 - acc: 0.9917 - val_loss: 0.0616 - val_acc: 0.9808\n",
      "Epoch 5/50\n",
      "79786/79786 [==============================] - 37s 470us/step - loss: 0.0137 - acc: 0.9951 - val_loss: 0.0683 - val_acc: 0.9811\n",
      "Epoch 6/50\n",
      "79786/79786 [==============================] - 38s 474us/step - loss: 0.0084 - acc: 0.9973 - val_loss: 0.0768 - val_acc: 0.9806\n",
      "Epoch 7/50\n",
      "79786/79786 [==============================] - 38s 470us/step - loss: 0.0055 - acc: 0.9984 - val_loss: 0.0878 - val_acc: 0.9806\n",
      "Training on fold # 1\n",
      "Train on 79786 samples, validate on 79785 samples\n",
      "Epoch 1/50\n",
      "79786/79786 [==============================] - 41s 512us/step - loss: 0.0943 - acc: 0.9733 - val_loss: 0.0518 - val_acc: 0.9816\n",
      "Epoch 2/50\n",
      "79786/79786 [==============================] - 37s 464us/step - loss: 0.0442 - acc: 0.9835 - val_loss: 0.0511 - val_acc: 0.9821\n",
      "Epoch 3/50\n",
      "79786/79786 [==============================] - 37s 469us/step - loss: 0.0323 - acc: 0.9877 - val_loss: 0.0518 - val_acc: 0.9814\n",
      "Epoch 4/50\n",
      "79786/79786 [==============================] - 37s 467us/step - loss: 0.0215 - acc: 0.9919 - val_loss: 0.0601 - val_acc: 0.9814\n",
      "Epoch 5/50\n",
      "79786/79786 [==============================] - 38s 475us/step - loss: 0.0135 - acc: 0.9954 - val_loss: 0.0699 - val_acc: 0.9808\n",
      "Epoch 6/50\n",
      "79786/79786 [==============================] - 37s 468us/step - loss: 0.0088 - acc: 0.9973 - val_loss: 0.0775 - val_acc: 0.9803\n",
      "Epoch 7/50\n",
      "79786/79786 [==============================] - 38s 473us/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0848 - val_acc: 0.9806\n"
     ]
    }
   ],
   "source": [
    "models, val_loss = train_folds(train_data, train_labels, 2, 256, get_text_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall val-loss: 0.05054252390620358\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall val-loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the predections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting testing results...\n",
      "153164/153164 [==============================] - 8s 52us/step\n",
      "153164/153164 [==============================] - 7s 47us/step\n"
     ]
    }
   ],
   "source": [
    "#test_data = test_df\n",
    "CLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "submit_path_prefix = \"ToxicNN-\" + str(MAX_SEQUENCE_LENGTH) \n",
    "\n",
    "print(\"Predicting testing results...\")\n",
    "test_predicts_list = []\n",
    "for fold_id, model in enumerate(models):\n",
    "    test_predicts = model.predict(test_data, batch_size=256, verbose=1)\n",
    "    test_predicts_list.append(test_predicts)\n",
    "\n",
    "# merge each folds' predictions by averaging\n",
    "test_predicts = np.zeros(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts += fold_predict\n",
    "test_predicts /= len(test_predicts_list)\n",
    "\n",
    "# create the submission file\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\n",
    "test_predicts[\"id\"] = test_ids\n",
    "test_predicts = test_predicts[[\"id\"] + CLASSES]\n",
    "submit_path = submit_path_prefix + \"-Loss{:4f}.csv\".format(val_loss)\n",
    "test_predicts.to_csv(submit_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
